{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced policy (Minimize steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adds one each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dynamic_programming import policy_improve, policy_iteration, generate_deterministic_policy, deterministic_policy_eval_step, policy_improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_actions = np.load('states_actions.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(V, states_actions):\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        actions_list = list(actions.keys())\n",
    "        expected_rewards = np.zeros(len(actions_list))\n",
    "        for i, (action, data) in enumerate(actions.items()):\n",
    "            next_state = data['next_node']\n",
    "            # This is the modification\n",
    "            winner = data['winner']\n",
    "            reward = winner * 12\n",
    "            if winner == 0:\n",
    "                expected_rewards[i] = - (V[next_state]) - np.sign(-V[next_state])\n",
    "            else:\n",
    "                # Esto es un nodo terminal\n",
    "                expected_rewards[i] = - reward\n",
    "        pi[state] = actions_list[np.argmax(expected_rewards)]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_policy_eval_step(states_actions, V, pi):\n",
    "    # Evaluation in place (in contrast with evaluation with 2 arrays).\n",
    "    # Needs less memory and converges too\n",
    "    # pi is a dict and pi[s] is the best action for that state. (The most probable action)\n",
    "    delta = 0\n",
    "    for state, actions in states_actions.items():\n",
    "        V_updated = 0\n",
    "        action = pi[state]\n",
    "        next_node = actions[action]['next_node']\n",
    "        # This is the modification\n",
    "        winner = actions[action]['winner']\n",
    "        reward = winner * 12\n",
    "        if winner == 0:\n",
    "            V_updated = V_updated + -(V[next_node]) - np.sign(-V[next_node])\n",
    "        else:\n",
    "            # Esto es un nodo terminal\n",
    "            V_updated = V_updated - reward\n",
    "        delta = max(delta, np.abs(V_updated - V[state]))\n",
    "        V[state] = V_updated \n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 23097\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 6595\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 1696\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 400\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 80\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 12\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_policy = generate_deterministic_policy(states_actions)\n",
    "optimum_policy, optimum_V = policy_iteration(states_actions, \n",
    "                                             initial_policy, \n",
    "                                             deterministic_policy_eval_step, \n",
    "                                             policy_improve,\n",
    "                                             verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_1\n",
    "first_player = 1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Value_func_steps_reward', optimum_V)\n",
    "np.save('pi_func_steps_reward', optimum_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets play game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "import numpy as np\n",
    "\n",
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_policy = np.load('pi_func_steps_reward.npy').item()\n",
    "evalPolicy = EvaluatePolicy(optimum_policy)\n",
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces):\n",
    "    print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "    print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "    print('ties:', str(int(100*ties/episodes + 0.5))+ '%')\n",
    "    print('Max, Mean, Min margins: ', end ='')\n",
    "    print(np.max(margins), np.mean(margins), np.min(margins))\n",
    "    print('Max, Mean, Min steps: ', end ='')\n",
    "    print(np.max(steps_array), np.mean(steps_array), np.min(steps_array))\n",
    "    print('Max, Mean, Min pieces: ', end ='')\n",
    "    print(np.max(pieces), np.mean(pieces), np.min(pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -3 -8.6988 -12\n",
      "Max, Mean, Min steps: 12 9.0443 8\n",
      "Max, Mean, Min pieces: 16 12.2076 11\n"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 81%\n",
      "player_2 wins: 19%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: 8 1.433 -9\n",
      "Max, Mean, Min steps: 14 12.489 10\n",
      "Max, Mean, Min pieces: 16 15.931 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
