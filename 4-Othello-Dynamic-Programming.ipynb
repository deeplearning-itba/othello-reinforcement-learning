{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "from tree_search_algs import bfs_cannonical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Search to find all states\n",
    "Note: we are finding all states given that player 1 (white) starts playing. \n",
    "States of player 2 are expressed in cannonical form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.4 s, sys: 385 ms, total: 55.8 s\n",
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "# Find all states of game doing a search tree\n",
    "first_player = 1\n",
    "%time states_actions_player_1 = bfs_cannonical(game, board, first_player) # (1 white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.3 s, sys: 352 ms, total: 55.6 s\n",
      "Wall time: 55.7 s\n"
     ]
    }
   ],
   "source": [
    "# Find all states of game doing a search tree\n",
    "first_player = -1\n",
    "%time states_actions_player_2 = bfs_cannonical(game, board, first_player) #(-1 black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_actions = {**states_actions_player_1, **states_actions_player_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 53651\n",
      "Number of states: 53651\n"
     ]
    }
   ],
   "source": [
    "print('Number of states:', len(states_actions_player_1))\n",
    "print('Number of states:', len(states_actions_player_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 91282\n"
     ]
    }
   ],
   "source": [
    "print('Number of states:', len(states_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('states_actions', states_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 julianganzabal  staff   144M Feb 23 12:31 states_actions.npy\n"
     ]
    }
   ],
   "source": [
    "# Size of the file. Only works if linux or mac. Try dir for windows\n",
    "! ls -lah states_actions.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "\n",
      "Actions, rewards and next nodes of state:\n",
      "1 {'reward': 0, 'next_node': (0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)}\n",
      "4 {'reward': 0, 'next_node': (0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)}\n",
      "11 {'reward': 0, 'next_node': (0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0)}\n",
      "14 {'reward': 0, 'next_node': (0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0)}\n"
     ]
    }
   ],
   "source": [
    "state = list(states_actions.keys())[0]\n",
    "print('Example of state:')\n",
    "print(state)\n",
    "print()\n",
    "print('Actions, rewards and next nodes of state:')\n",
    "for k, v in states_actions[state].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 1, 0, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5c32d8cba912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m [ 0,  0,  0,  0]])\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstates_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: (0, 1, 0, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)"
     ]
    }
   ],
   "source": [
    "# Board after player 1 plays action 1\n",
    "board_1 = np.array(\n",
    "[[0,  1,  0,  0],\n",
    "[ 0,  1,  1,  0],\n",
    "[ 0,  1, -1,  0],\n",
    "[ 0,  0,  0,  0]])\n",
    "states_actions[tuple(board_1.reshape(-1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why the above is not a valid state? Which is the state for the Board after player 1 plays action 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a uniform stochastic policy\n",
    "It is used as initial policy to test stochastic policy evalution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "Actions, Probabilities:\n",
      "1 0.25\n",
      "4 0.25\n",
      "11 0.25\n",
      "14 0.25\n",
      "------------------\n",
      "Example of state:\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)\n",
      "Actions, Probabilities:\n",
      "0 0.3333333333333333\n",
      "2 0.3333333333333333\n",
      "8 0.3333333333333333\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "from dynamic_programming import generate_uniform_stochastic_policy\n",
    "pi = generate_uniform_stochastic_policy(states_actions)\n",
    "\n",
    "for i in range(2):\n",
    "    state = list(pi.keys())[i]\n",
    "    print('Example of state:')\n",
    "    print(state)\n",
    "    print('Actions, Probabilities:')\n",
    "    for k, v in pi[state].items():\n",
    "        print(k, v)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation test\n",
    "### Stochastic policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import policy_evaluation, stochastic_policy_eval_step\n",
    "# stochastic_policy_eval_step does policy_evaluation using probabilities. Check library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n"
     ]
    }
   ],
   "source": [
    "# Run it multiple times to check it takes different number of iterations to converge\n",
    "stochastic_pi = generate_uniform_stochastic_policy(states_actions)\n",
    "V_stochastic, iters = policy_evaluation(stochastic_policy_eval_step, \n",
    "                             states_actions, \n",
    "                             stochastic_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 examples of Value function:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.20959130704875395\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 0.20959130704875395\n",
      "(0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 0.20959130704875395\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0) 0.20959130704875395\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0) 0.209591307048754\n",
      "(-1, 1, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.7356919474451302\n",
      "(0, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 0.0992612276020233\n",
      "(0, 1, 0, 0, 0, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 0.007656798696845005\n",
      "(-1, 0, 0, 0, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.7356919474451302\n",
      "(0, 0, -1, 0, 1, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 0.007656798696845005\n",
      "(0, 0, 0, 0, 1, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 0.0992612276020233\n"
     ]
    }
   ],
   "source": [
    "print('10 examples of Value function:')\n",
    "i = 0 \n",
    "for k, v in V_stochastic.items():\n",
    "    print(k, v)\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import generate_deterministic_policy, deterministic_policy_eval_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state: (0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "\"best\" action: 14\n",
      "------------------\n",
      "Example of state: (0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)\n",
      "\"best\" action: 2\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "deterministic_pi = generate_deterministic_policy(states_actions)\n",
    "for i in range(2):\n",
    "    state = list(pi.keys())[i]\n",
    "    print('Example of state:', state)\n",
    "    print('\"best\" action:', deterministic_pi[state])\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n"
     ]
    }
   ],
   "source": [
    "# Run it multiple times to check it always takes the same number of iterations\n",
    "V_deterministic, iters = policy_evaluation(deterministic_policy_eval_step, \n",
    "                             states_actions, \n",
    "                             deterministic_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 examples of Value function:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 0.2\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) -1\n",
      "(0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0) -0.2\n",
      "(-1, 1, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 1\n",
      "(0, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 1\n",
      "(0, 1, 0, 0, 0, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 1\n",
      "(-1, 0, 0, 0, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -1\n",
      "(0, 0, -1, 0, 1, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, 1, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) -1\n"
     ]
    }
   ],
   "source": [
    "print('10 examples of Value function:')\n",
    "i = 0 \n",
    "for k, v in V_deterministic.items():\n",
    "    print(k, v)\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See values are just 1 or -1 (They could be +-0.2 for ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import policy_improve, policy_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 23051\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 3996\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 1172\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 293\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 72\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 23\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 9\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 6\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 2\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 2\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 1\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_policy = generate_deterministic_policy(states_actions)\n",
    "optimum_policy, optimum_V = policy_iteration(states_actions, initial_policy, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_1\n",
    "first_player = 1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result indicates that player 1 always looses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_2\n",
    "first_player = -1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Value_func', optimum_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pi_func', optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 julianganzabal  staff    76M Feb 23 12:46 Value_func.npy\n",
      "-rw-r--r--  1 julianganzabal  staff    80M Feb 23 12:46 pi_func.npy\n",
      "-rw-r--r--  1 julianganzabal  staff   144M Feb 23 12:31 states_actions.npy\n"
     ]
    }
   ],
   "source": [
    "! ls -lah *.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets play game with optimun policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalPolicy = EvaluatePolicy(optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random vs Random as and Greedy vs Greedy as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 38%\n",
      "player_2 wins: 53%\n",
      "ties: 9%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second player seems to have more probability to win playing random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 44%\n",
      "player_2 wins: 50%\n",
      "ties: 6%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.greedy_player, -1: evalPolicy.greedy_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal policy always win as second player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 82%\n",
      "player_2 wins: 18%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtimal policy can't win all but does much better than random player 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 67%\n",
      "player_2 wins: 33%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.greedy_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also does good as player 1 against greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
