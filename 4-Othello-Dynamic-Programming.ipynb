{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "from tree_search_algs import bfs_cannonical\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.7 s, sys: 257 ms, total: 28.9 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%time states_actions = bfs_cannonical(game, board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(states_actions), list(states_actions.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_policy(states_actions):\n",
    "    # Cada estado contienen un dict con las probs de cada acción\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        pi[state] = {}\n",
    "        prob = 1/len(actions)\n",
    "        for action, data in actions.items():\n",
    "            pi[state][action] = prob\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para probar imprimir resultados de policy\n",
    "# pi = generate_random_policy(states_actions)\n",
    "# len(pi), list(pi.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit\n",
    "def policy_eval_step_random(states_actions, V, pi):\n",
    "    # In place\n",
    "    # Pi[s] posee la probabilidad de cada acción\n",
    "    delta = 0\n",
    "    for state, actions in states_actions.items():\n",
    "        V_updated = 0\n",
    "        for action, data in actions.items():\n",
    "            next_node = data['next_node']\n",
    "            reward = data['reward']\n",
    "            prob = pi[state][action]\n",
    "            if reward == 0:\n",
    "                V_updated = V_updated + prob*(- V[next_node])\n",
    "            else:\n",
    "                # Esto es un nodo terminal\n",
    "                V_updated = V_updated - prob*reward\n",
    "        delta = max(delta, np.abs(V_updated - V[state]))\n",
    "        V[state] = V_updated\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy_eval_step, states_actions, pi, theta, verbose=0):\n",
    "    # In place\n",
    "    # Pi[s] posee la probabilidad de cada acción\n",
    "    if verbose:\n",
    "        print('Iteración numero:')\n",
    "    \n",
    "    V = {}\n",
    "    iters = 0\n",
    "    for state in states_actions:\n",
    "        V[state] = 0\n",
    "    delta = theta + 1\n",
    "    while theta<delta: \n",
    "        V, delta = policy_eval_step(states_actions, V, pi)\n",
    "        iters += 1\n",
    "        if verbose:\n",
    "            print(iters, end=' ')\n",
    "    print()\n",
    "    return V, iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n",
      "CPU times: user 1.08 s, sys: 0 ns, total: 1.08 s\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "initial_random_pi = generate_random_policy(states_actions)\n",
    "%time V, iters = policy_eval(policy_eval_step_random, states_actions, initial_random_pi, 1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomentar para ver resultados\n",
    "# len(V), iters, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_policy_rand(states_actions):\n",
    "    # Cada estado contienen una sola acción elegida aleatoriamente dentro de las posibles\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        pi[state] = np.random.choice(list(actions.keys()))\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_policy_first(states_actions):\n",
    "    # Cada estado contienen la primer acción de la lista\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        pi[state] = list(actions.keys())[0]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_step(states_actions, V, pi):\n",
    "    # In place\n",
    "    # Pi[s] posee la máxima acción\n",
    "    delta = 0\n",
    "    for state, actions in states_actions.items():\n",
    "        V_updated = 0\n",
    "        action = pi[state]\n",
    "        next_node = actions[action]['next_node']\n",
    "        reward = actions[action]['reward']\n",
    "        if reward == 0:\n",
    "            V_updated = V_updated + (-V[next_node])\n",
    "        else:\n",
    "            # Esto es un nodo terminal\n",
    "            V_updated = V_updated - reward\n",
    "        delta = max(delta, np.abs(V_updated - V[state]))\n",
    "        V[state] = V_updated\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "CPU times: user 670 ms, sys: 7.98 ms, total: 678 ms\n",
      "Wall time: 676 ms\n"
     ]
    }
   ],
   "source": [
    "initial_fixed_pi = generate_fixed_policy_first(states_actions)\n",
    "%time V_fixed, iters = policy_eval(policy_eval_step, states_actions, initial_fixed_pi, 1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 24729), (-1, 23819), (0.2, 2819), (-0.2, 2284)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(list(V_fixed.values())).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(V, states_actions):\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        actions_list = list(actions.keys())\n",
    "        expected_rewards = np.zeros(len(actions_list))\n",
    "        for i, (action, data) in enumerate(actions.items()):\n",
    "            next_state = data['next_node']\n",
    "            reward = data['reward']\n",
    "            if reward == 0:\n",
    "                expected_rewards[i] = - V[next_state]\n",
    "            else:\n",
    "                # Esto es un nodo terminal\n",
    "                expected_rewards[i] = - reward\n",
    "        pi[state] = actions_list[np.argmax(expected_rewards)]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 12703\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 2207\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 619\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 184\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 47\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 16\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 4\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 4\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 3\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 2\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 1\n",
      "---------------------------\n",
      "Iteración numero:\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(pi_old, verbose = 0):\n",
    "    # Politica inicial\n",
    "    policy_updates = 100\n",
    "    while policy_updates > 0:\n",
    "        # Calculo values de politica\n",
    "        V, iters = policy_eval(policy_eval_step, states_actions, pi_old, 1e-6, verbose=verbose)\n",
    "        # Mejoro política con values\n",
    "        pi = policy_improve(V, states_actions)\n",
    "\n",
    "        policy_updates = 0\n",
    "        for j, (state, accion) in enumerate(pi.items()):\n",
    "            if accion != pi_old[state]:\n",
    "                 policy_updates += 1\n",
    "        pi_old = pi.copy()\n",
    "        if verbose:\n",
    "            print('Cantidad de diferencias de la vieja politica con la nueva:', policy_updates)\n",
    "            print('---------------------------')\n",
    "    return pi_old, V\n",
    "\n",
    "#initial_policy = generate_fixed_policy_first(states_actions)\n",
    "initial_policy = generate_fixed_policy_rand(states_actions)\n",
    "final_policy, V = policy_iteration(initial_policy, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jugamos con la policy obtenida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalPolicy = EvaluatePolicy(final_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que la politica gana siempre si juega primero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 81%\n",
      "player_2 wins: 19%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 69%\n",
      "player_2 wins: 32%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.greedy_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
