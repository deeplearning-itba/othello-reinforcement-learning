{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "from tree_search_algs import bfs_cannonical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Search to find all states\n",
    "Note: we are finding all states given that player 1 (white) starts playing. \n",
    "States of player 2 are expressed in cannonical form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 s, sys: 167 ms, total: 29.6 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "# Find all states of game doing a search tree\n",
    "first_player = 1\n",
    "%time states_actions_player_1 = bfs_cannonical(game, board, first_player) # (1 white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 s, sys: 248 ms, total: 29.2 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "# Find all states of game doing a search tree\n",
    "first_player = -1\n",
    "%time states_actions_player_2 = bfs_cannonical(game, board, first_player) #(-1 black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_actions = {**states_actions_player_1, **states_actions_player_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 53651\n",
      "Number of states: 53651\n"
     ]
    }
   ],
   "source": [
    "print('Number of states:', len(states_actions_player_1))\n",
    "print('Number of states:', len(states_actions_player_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 91282\n"
     ]
    }
   ],
   "source": [
    "print('Number of states:', len(states_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('states_actions', states_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 usuario usuario 144M feb 23 02:47 states_actions.npy\n"
     ]
    }
   ],
   "source": [
    "# Los estados ocupan 144M\n",
    "% ls -lah sta*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "\n",
      "Actions, rewards and next nodes of state:\n",
      "1 {'reward': 0, 'next_node': (0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)}\n",
      "4 {'reward': 0, 'next_node': (0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)}\n",
      "11 {'reward': 0, 'next_node': (0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0)}\n",
      "14 {'reward': 0, 'next_node': (0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0)}\n"
     ]
    }
   ],
   "source": [
    "state = list(states_actions.keys())[0]\n",
    "print('Example of state:')\n",
    "print(state)\n",
    "print()\n",
    "print('Actions, rewards and next nodes of state:')\n",
    "for k, v in states_actions[state].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 1, 0, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-5c32d8cba912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m [ 0,  0,  0,  0]])\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstates_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: (0, 1, 0, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)"
     ]
    }
   ],
   "source": [
    "# Board after player 1 plays action 1\n",
    "board_1 = np.array(\n",
    "[[0,  1,  0,  0],\n",
    "[ 0,  1,  1,  0],\n",
    "[ 0,  1, -1,  0],\n",
    "[ 0,  0,  0,  0]])\n",
    "states_actions[tuple(board_1.reshape(-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0],\n",
       "       [ 0, -1,  1,  0],\n",
       "       [ 0,  1, -1,  0],\n",
       "       [ 0,  0,  0,  0]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'reward': 0,\n",
       "  'next_node': (0, -1, 1, 0, 0, -1, 1, 0, 0, -1, 1, 0, 0, 0, 0, 0)},\n",
       " 3: {'reward': 0,\n",
       "  'next_node': (0, 0, 1, -1, 0, 1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)},\n",
       " 11: {'reward': 0,\n",
       "  'next_node': (0, 0, 1, 0, 0, 1, 1, 0, 0, -1, -1, -1, 0, 0, 0, 0)}}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board_2 = np.array(\n",
    "[[0,  0,  -1,  0],\n",
    "[ 0,  -1,  -1,  0],\n",
    "[ 0,  1, -1,  0],\n",
    "[ 0,  0,  0,  0]])\n",
    "states_actions[tuple(board_2.reshape(-1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why the above is not a valid state? Which is the state for the Board after player 1 plays action 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a uniform stochastic policy\n",
    "It is used as initial policy to test stochastic policy evalution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "Actions, Probabilities:\n",
      "1 0.25\n",
      "4 0.25\n",
      "11 0.25\n",
      "14 0.25\n",
      "------------------\n",
      "Example of state:\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)\n",
      "Actions, Probabilities:\n",
      "0 0.3333333333333333\n",
      "2 0.3333333333333333\n",
      "8 0.3333333333333333\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "from dynamic_programming import generate_uniform_stochastic_policy\n",
    "pi = generate_uniform_stochastic_policy(states_actions)\n",
    "\n",
    "for i in range(2):\n",
    "    state = list(pi.keys())[i]\n",
    "    print('Example of state:')\n",
    "    print(state)\n",
    "    print('Actions, Probabilities:')\n",
    "    for k, v in pi[state].items():\n",
    "        print(k, v)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation test\n",
    "### Stochastic policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import policy_evaluation, stochastic_policy_eval_step\n",
    "# stochastic_policy_eval_step does policy_evaluation using probabilities. Check library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n"
     ]
    }
   ],
   "source": [
    "# Run it multiple times to check it takes different number of iterations to converge\n",
    "stochastic_pi = generate_uniform_stochastic_policy(states_actions)\n",
    "V_stochastic, iters = policy_evaluation(stochastic_policy_eval_step, \n",
    "                             states_actions, \n",
    "                             stochastic_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 examples of Value function:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.20959130704875395\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 0.20959130704875395\n",
      "(0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 0.20959130704875395\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0) 0.20959130704875395\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0) 0.209591307048754\n",
      "(-1, 1, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.7356919474451302\n",
      "(0, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 0.0992612276020233\n",
      "(0, 1, 0, 0, 0, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 0.007656798696845005\n",
      "(-1, 0, 0, 0, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.7356919474451302\n",
      "(0, 0, -1, 0, 1, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 0.007656798696845005\n",
      "(0, 0, 0, 0, 1, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 0.0992612276020233\n"
     ]
    }
   ],
   "source": [
    "print('10 examples of Value function:')\n",
    "i = 0 \n",
    "for k, v in V_stochastic.items():\n",
    "    print(k, v)\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import generate_deterministic_policy, deterministic_policy_eval_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state: (0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "\"best\" action: 14\n",
      "------------------\n",
      "Example of state: (0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)\n",
      "\"best\" action: 8\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "deterministic_pi = generate_deterministic_policy(states_actions)\n",
    "for i in range(2):\n",
    "    state = list(pi.keys())[i]\n",
    "    print('Example of state:', state)\n",
    "    print('\"best\" action:', deterministic_pi[state])\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n"
     ]
    }
   ],
   "source": [
    "# Run it multiple times to check it always takes the same number of iterations\n",
    "V_deterministic, iters = policy_evaluation(deterministic_policy_eval_step, \n",
    "                             states_actions, \n",
    "                             deterministic_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 examples of Value function:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 1\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0) -1\n",
      "(-1, 1, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -1\n",
      "(0, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -1\n",
      "(0, 1, 0, 0, 0, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) -1\n",
      "(-1, 0, 0, 0, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -1\n",
      "(0, 0, -1, 0, 1, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, 1, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 1\n"
     ]
    }
   ],
   "source": [
    "print('10 examples of Value function:')\n",
    "i = 0 \n",
    "for k, v in V_deterministic.items():\n",
    "    print(k, v)\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See values are just 1 or -1 (They could be +-0.2 for ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import policy_improve, policy_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 23166\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 4278\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 1221\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 303\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 82\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 27\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 9\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 5\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 3\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 2\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 1\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Cantidad de diferencias de la vieja politica con la nueva: 0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_policy = generate_deterministic_policy(states_actions)\n",
    "optimum_policy, optimum_V = policy_iteration(states_actions, initial_policy, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_1\n",
    "print(optimum_V[tuple(board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result indicates that player 1 always looses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_2\n",
    "print(optimum_V[tuple(-board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Value_func', optimum_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pi_func', optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 usuario usuario  80M feb 23 02:50 pi_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario 144M feb 23 02:47 states_actions.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M feb 23 02:50 Value_func.npy\n"
     ]
    }
   ],
   "source": [
    "! ls -lah *.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets play game with optimun policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalPolicy = EvaluatePolicy(optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random vs Random as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 34%\n",
      "player_2 wins: 56%\n",
      "ties: 9%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second player seems to have more probability to win playing random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal policy always win as second player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 81%\n",
      "player_2 wins: 19%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtimal policy can't win all but does much better than random player 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 67%\n",
      "player_2 wins: 33%\n",
      "ties: 0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.greedy_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also does good as player 1 against greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
