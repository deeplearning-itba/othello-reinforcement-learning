{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration (Maximize Margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the reward being 1 for winning and -1 for loosing, the reward is the number of pieces winner is over looser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import base64\n",
    "from dynamic_programming import policy_improve, policy_iteration, generate_deterministic_policy, deterministic_policy_eval_step, policy_improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3316988"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shelve\n",
    "filename='./data/6by6/states'\n",
    "states_actions = shelve.open(filename, flag='r')\n",
    "len(states_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2147593\n",
    "# 3316988"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "node1 = 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA//////////8BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\\nAAAAAAAAAP////////////////////8BAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////////////\\n//////////////////8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAA////\\n/////////////////wAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAA\\nAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\\nAAAA\\n'\n",
    "node2 = 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD//////////wAAAAAAAAAAAAAAAAAAAAAA\\nAAAAAAAAAAEAAAAAAAAA/////////////////////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEA\\nAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////////////////////////\\n/////////////////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAA\\nAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAA//////////8AAAAAAAAAAAAAAAAA\\nAAAA\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, -1,  1,  0],\n",
       "       [ 0,  0, -1, -1,  1,  0],\n",
       "       [ 0, -1, -1, -1,  0,  0],\n",
       "       [ 0,  0,  1, -1, -1,  0],\n",
       "       [ 0,  1,  1,  0,  0,  0],\n",
       "       [ 1,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.frombuffer(base64.decodebytes(str.encode(node1)), dtype=int).reshape(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0, -1,  0],\n",
       "       [ 0,  0,  1, -1, -1,  0],\n",
       "       [ 0,  0,  1,  1,  0,  0],\n",
       "       [ 0, -1, -1, -1, -1,  0],\n",
       "       [ 0,  0,  1,  0,  1,  0],\n",
       "       [ 0,  1,  0, -1,  0,  0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.frombuffer(base64.decodebytes(str.encode(node2)), dtype=int).reshape(6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(states_actions, theta=1e-8, winning_reward=1e3):\n",
    "    V = {}\n",
    "    iters = 0\n",
    "    for state in states_actions:\n",
    "        V[state] = 0\n",
    "    delta = theta + 1\n",
    "    iterat = 0\n",
    "    N = len(states_actions)\n",
    "    while theta<delta: \n",
    "        suma = 0\n",
    "        delta = 0\n",
    "        for state, actions in states_actions.items():\n",
    "            expected_rewards = []\n",
    "            for action in actions:\n",
    "                next_state = actions[action]['next_node']\n",
    "                winner = actions[action]['winner']\n",
    "                next_state_matrix = np.frombuffer(base64.decodebytes(str.encode(next_state)), dtype=int).reshape(6, 6)\n",
    "                reward = next_state_matrix.sum()\n",
    "                if winner == 0:\n",
    "                    expected_rewards.append(-V[next_state])\n",
    "                else:\n",
    "                    # Esto es un nodo terminal\n",
    "                    expected_rewards.append(-reward)\n",
    "            V_updated = max(expected_rewards)\n",
    "            suma = suma + np.abs(V_updated - V[state])\n",
    "            delta = max(delta, np.abs(V_updated - V[state]))\n",
    "            V[state] = V_updated\n",
    "        iterat += 1\n",
    "        print(iterat, delta, suma/N)\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD//////////wAAAAAAAAAAAAAAAAAAAAAA\\nAAAAAAAAAAEAAAAAAAAA/////////////////////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEA\\nAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////////////////////////\\n/////////////////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAA\\nAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAA//////////8AAAAAAAAAAAAAAAAA\\nAAAA\\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-d1a96f96132f>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(states_actions, theta, winning_reward)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mwinner\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0mexpected_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0;31m# Esto es un nodo terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD//////////wAAAAAAAAAAAAAAAAAAAAAA\\nAAAAAAAAAAEAAAAAAAAA/////////////////////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEA\\nAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////////////////////////\\n/////////////////wAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAABAAAA\\nAAAAAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAA//////////8AAAAAAAAAAAAAAAAA\\nAAAA\\n'"
     ]
    }
   ],
   "source": [
    "%time V, delta = value_iteration(states_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(V, states_actions):\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        actions_list = list(actions.keys())\n",
    "        expected_rewards = np.zeros(len(actions_list))\n",
    "        for i, (action, data) in enumerate(actions.items()):\n",
    "            next_state = data['next_node']\n",
    "            # This is the modification\n",
    "            winner = data['winner']\n",
    "            reward = np.array(next_state).sum()\n",
    "            if winner == 0:\n",
    "                expected_rewards[i] = - V[next_state]\n",
    "            else:\n",
    "                # Esto es un nodo terminal\n",
    "                expected_rewards[i] = - reward\n",
    "        pi[state] = actions_list[np.argmax(expected_rewards)]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 879 ms, sys: 206 ms, total: 1.09 s\n",
      "Wall time: 888 ms\n"
     ]
    }
   ],
   "source": [
    "%time pi = policy_improve(V, states_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_policy = pi.copy()\n",
    "optimum_V = V.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_1\n",
    "first_player = 1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Value_func_margin_reward_val_iter', optimum_V)\n",
    "np.save('pi_func_margin_reward_val_iter', optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 usuario usuario  18M mar 16 15:49 chess_min_steps_pi.npy\n",
      "-rw-rw-r-- 1 usuario usuario  18M mar 16 16:38 chess_min_steps_pi_value_iter.npy\n",
      "-rw-rw-r-- 1 usuario usuario  32M mar 16 15:49 chess_min_steps_V.npy\n",
      "-rw-rw-r-- 1 usuario usuario  32M mar 16 16:38 chess_min_steps_V_value_iter.npy\n",
      "-rw-rw-r-- 1 usuario usuario  18M mar 16 17:17 chess_pi.npy\n",
      "-rw-rw-r-- 1 usuario usuario  14M mar 16 17:17 chess_V.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 12:46 pi_func_margin_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 19 11:15 pi_func_margin_reward_val_iter.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 13:04 pi_func_min_pieces_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M feb 23 02:50 pi_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 12:40 pi_func_only_winner.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 15:12 pi_func_steps_reward.npy\n",
      "-rw-r--r-- 1 usuario usuario 272M mar 16 10:17 rook_final.npy\n",
      "-rw-rw-r-- 1 usuario usuario 144M mar 16 12:38 states_actions.npy\n",
      "-rw-rw-r-- 1 usuario usuario  81M mar 16 12:45 Value_func_margin_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  81M mar 19 11:15 Value_func_margin_reward_val_iter.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 13:04 Value_func_min_pieces_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M feb 23 02:50 Value_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M mar 16 12:39 Value_func_only_winner.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 15:12 Value_func_steps_reward.npy\n"
     ]
    }
   ],
   "source": [
    "!ls -lah *.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets play game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "import numpy as np\n",
    "\n",
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_policy = np.load('pi_func_margin_reward_val_iter.npy').item()\n",
    "evalPolicy = EvaluatePolicy(optimum_policy)\n",
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces):\n",
    "    print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "    print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "    print('ties:', str(int(100*ties/episodes + 0.5))+ '%')\n",
    "    print('Max, Mean, Min margins: ', end ='')\n",
    "    print(np.max(margins), np.mean(margins), np.min(margins))\n",
    "    print('Max, Mean, Min steps: ', end ='')\n",
    "    print(np.max(steps_array), np.mean(steps_array), np.min(steps_array))\n",
    "    print('Max, Mean, Min pieces: ', end ='')\n",
    "    print(np.max(pieces), np.mean(pieces), np.min(pieces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -8 -13.449 -16\n",
      "Max, Mean, Min steps: 16 13.812 10\n",
      "Max, Mean, Min pieces: 16 15.667 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 96%\n",
      "player_2 wins: 2%\n",
      "ties: 2%\n",
      "Max, Mean, Min margins: 15 8.624 -8\n",
      "Max, Mean, Min steps: 15 13.071 10\n",
      "Max, Mean, Min pieces: 16 15.332 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player1: Optumin Policy (Margin), Player2: Optimun Policy (only win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chess_min_steps_pi.npy\t\t    pi_func_only_winner.npy\n",
      "chess_min_steps_pi_value_iter.npy   pi_func_steps_reward.npy\n",
      "chess_min_steps_V.npy\t\t    rook_final.npy\n",
      "chess_min_steps_V_value_iter.npy    states_actions.npy\n",
      "chess_pi.npy\t\t\t    Value_func_margin_reward.npy\n",
      "chess_V.npy\t\t\t    Value_func_margin_reward_val_iter.npy\n",
      "pi_func_margin_reward.npy\t    Value_func_min_pieces_reward.npy\n",
      "pi_func_margin_reward_val_iter.npy  Value_func.npy\n",
      "pi_func_min_pieces_reward.npy\t    Value_func_only_winner.npy\n",
      "pi_func.npy\t\t\t    Value_func_steps_reward.npy\n"
     ]
    }
   ],
   "source": [
    "!ls *.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_margin = np.load('pi_func_margin_reward.npy').item(0)\n",
    "pi_only_wins = np.load('pi_func_only_winner.npy').item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum_policy = np.load('pi_func_only_winner.npy').item()\n",
    "evalPolicy = EvaluatePolicy(pi_margin, pi_only_wins)\n",
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -2 -2.0 -2\n",
      "Max, Mean, Min steps: 13 13.0 13\n",
      "Max, Mean, Min pieces: 16 16.0 16\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.policy_player_pi2}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has no sense to play more than once because they are deterministic policies\n",
    "- Very interesting to note that player 2 wins but with a margin of only -2, the minimum margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player1: Optimun Policy (only win), Player2: Optumin Policy (Margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -9 -9.0 -9\n",
      "Max, Mean, Min steps: 10 10.0 10\n",
      "Max, Mean, Min pieces: 13 13.0 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player_pi2, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has no sense to play more than once because they are deterministic policies\n",
    "- Margin policy as second player wins by a margin of 9. Es expected more or equal than 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
