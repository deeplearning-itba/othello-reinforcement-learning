{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration (Maximize Margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the reward being 1 for winning and -1 for loosing, the reward is the number of pieces winner is over looser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dynamic_programming import policy_improve, policy_iteration, generate_deterministic_policy, deterministic_policy_eval_step, policy_improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_actions = np.load('states_actions.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(states_actions, theta=1e-8, winning_reward=1e3):\n",
    "    V = {}\n",
    "    iters = 0\n",
    "    for state in states_actions:\n",
    "        V[state] = 0\n",
    "    delta = theta + 1\n",
    "    iterat = 0\n",
    "    N = len(states_actions)\n",
    "    while theta<delta: \n",
    "        suma = 0\n",
    "        delta = 0\n",
    "        for state, actions in states_actions.items():\n",
    "            expected_rewards = []\n",
    "            for action in actions:\n",
    "                next_state = actions[action]['next_node']\n",
    "                winner = actions[action]['winner']\n",
    "                reward = np.array(next_state).sum()\n",
    "                if winner == 0:\n",
    "                    expected_rewards.append(-V[next_state])\n",
    "                else:\n",
    "                    # Esto es un nodo terminal\n",
    "                    expected_rewards.append(-reward)\n",
    "            V_updated = max(expected_rewards)\n",
    "            suma = suma + np.abs(V_updated - V[state])\n",
    "            delta = max(delta, np.abs(V_updated - V[state]))\n",
    "            V[state] = V_updated\n",
    "        iterat += 1\n",
    "        print(iterat, delta, suma/N)\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16 1.3893429153611885\n",
      "2 16 1.8213996187638308\n",
      "3 16 1.6185995048311825\n",
      "4 16 1.1144913564558183\n",
      "5 16 0.632315242873732\n",
      "6 16 0.26934116255121493\n",
      "7 16 0.115093884884205\n",
      "8 14 0.03661181832124625\n",
      "9 14 0.013003658990819658\n",
      "10 9 0.004108148375364256\n",
      "11 8 0.0015994391008084835\n",
      "12 8 0.00010955062334304682\n",
      "13 0 0.0\n",
      "CPU times: user 9.86 s, sys: 3.25 ms, total: 9.86 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%time V, delta = value_iteration(states_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(V, states_actions):\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        actions_list = list(actions.keys())\n",
    "        expected_rewards = np.zeros(len(actions_list))\n",
    "        for i, (action, data) in enumerate(actions.items()):\n",
    "            next_state = data['next_node']\n",
    "            # This is the modification\n",
    "            winner = data['winner']\n",
    "            reward = np.array(next_state).sum()\n",
    "            if winner == 0:\n",
    "                expected_rewards[i] = - V[next_state]\n",
    "            else:\n",
    "                # Esto es un nodo terminal\n",
    "                expected_rewards[i] = - reward\n",
    "        pi[state] = actions_list[np.argmax(expected_rewards)]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 879 ms, sys: 206 ms, total: 1.09 s\n",
      "Wall time: 888 ms\n"
     ]
    }
   ],
   "source": [
    "%time pi = policy_improve(V, states_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_policy = pi.copy()\n",
    "optimum_V = V.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_1\n",
    "first_player = 1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Value_func_margin_reward_val_iter', optimum_V)\n",
    "np.save('pi_func_margin_reward_val_iter', optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 usuario usuario  18M mar 16 15:49 chess_min_steps_pi.npy\n",
      "-rw-rw-r-- 1 usuario usuario  18M mar 16 16:38 chess_min_steps_pi_value_iter.npy\n",
      "-rw-rw-r-- 1 usuario usuario  32M mar 16 15:49 chess_min_steps_V.npy\n",
      "-rw-rw-r-- 1 usuario usuario  32M mar 16 16:38 chess_min_steps_V_value_iter.npy\n",
      "-rw-rw-r-- 1 usuario usuario  18M mar 16 17:17 chess_pi.npy\n",
      "-rw-rw-r-- 1 usuario usuario  14M mar 16 17:17 chess_V.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 12:46 pi_func_margin_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 19 11:15 pi_func_margin_reward_val_iter.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 13:04 pi_func_min_pieces_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M feb 23 02:50 pi_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 12:40 pi_func_only_winner.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 15:12 pi_func_steps_reward.npy\n",
      "-rw-r--r-- 1 usuario usuario 272M mar 16 10:17 rook_final.npy\n",
      "-rw-rw-r-- 1 usuario usuario 144M mar 16 12:38 states_actions.npy\n",
      "-rw-rw-r-- 1 usuario usuario  81M mar 16 12:45 Value_func_margin_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  81M mar 19 11:15 Value_func_margin_reward_val_iter.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 13:04 Value_func_min_pieces_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M feb 23 02:50 Value_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M mar 16 12:39 Value_func_only_winner.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 15:12 Value_func_steps_reward.npy\n"
     ]
    }
   ],
   "source": [
    "!ls -lah *.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets play game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "import numpy as np\n",
    "\n",
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_policy = np.load('pi_func_margin_reward_val_iter.npy').item()\n",
    "evalPolicy = EvaluatePolicy(optimum_policy)\n",
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces):\n",
    "    print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "    print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "    print('ties:', str(int(100*ties/episodes + 0.5))+ '%')\n",
    "    print('Max, Mean, Min margins: ', end ='')\n",
    "    print(np.max(margins), np.mean(margins), np.min(margins))\n",
    "    print('Max, Mean, Min steps: ', end ='')\n",
    "    print(np.max(steps_array), np.mean(steps_array), np.min(steps_array))\n",
    "    print('Max, Mean, Min pieces: ', end ='')\n",
    "    print(np.max(pieces), np.mean(pieces), np.min(pieces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -8 -13.449 -16\n",
      "Max, Mean, Min steps: 16 13.812 10\n",
      "Max, Mean, Min pieces: 16 15.667 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 96%\n",
      "player_2 wins: 2%\n",
      "ties: 2%\n",
      "Max, Mean, Min margins: 15 8.624 -8\n",
      "Max, Mean, Min steps: 15 13.071 10\n",
      "Max, Mean, Min pieces: 16 15.332 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player1: Optumin Policy (Margin), Player2: Optimun Policy (only win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chess_min_steps_pi.npy\t\t    pi_func_only_winner.npy\n",
      "chess_min_steps_pi_value_iter.npy   pi_func_steps_reward.npy\n",
      "chess_min_steps_V.npy\t\t    rook_final.npy\n",
      "chess_min_steps_V_value_iter.npy    states_actions.npy\n",
      "chess_pi.npy\t\t\t    Value_func_margin_reward.npy\n",
      "chess_V.npy\t\t\t    Value_func_margin_reward_val_iter.npy\n",
      "pi_func_margin_reward.npy\t    Value_func_min_pieces_reward.npy\n",
      "pi_func_margin_reward_val_iter.npy  Value_func.npy\n",
      "pi_func_min_pieces_reward.npy\t    Value_func_only_winner.npy\n",
      "pi_func.npy\t\t\t    Value_func_steps_reward.npy\n"
     ]
    }
   ],
   "source": [
    "!ls *.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_margin = np.load('pi_func_margin_reward.npy').item(0)\n",
    "pi_only_wins = np.load('pi_func_only_winner.npy').item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum_policy = np.load('pi_func_only_winner.npy').item()\n",
    "evalPolicy = EvaluatePolicy(pi_margin, pi_only_wins)\n",
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -2 -2.0 -2\n",
      "Max, Mean, Min steps: 13 13.0 13\n",
      "Max, Mean, Min pieces: 16 16.0 16\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.policy_player_pi2}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has no sense to play more than once because they are deterministic policies\n",
    "- Very interesting to note that player 2 wins but with a margin of only -2, the minimum margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player1: Optimun Policy (only win), Player2: Optumin Policy (Margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -9 -9.0 -9\n",
      "Max, Mean, Min steps: 10 10.0 10\n",
      "Max, Mean, Min pieces: 13 13.0 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player_pi2, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has no sense to play more than once because they are deterministic policies\n",
    "- Margin policy as second player wins by a margin of 9. Es expected more or equal than 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
