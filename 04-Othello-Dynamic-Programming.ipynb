{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "from tree_search_algs import bfs_cannonical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Search to find all states\n",
    "Note: we are finding all states given that player 1 (white) starts playing. \n",
    "States of player 2 are expressed in cannonical form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.6 s, sys: 342 ms, total: 30.9 s\n",
      "Wall time: 30.3 s\n"
     ]
    }
   ],
   "source": [
    "# Find all states of game doing a search tree if player 1 starts\n",
    "first_player = 1\n",
    "%time states_actions_player_1 = bfs_cannonical(game, board, first_player) # (1 white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.6 s, sys: 462 ms, total: 31.1 s\n",
      "Wall time: 30.4 s\n"
     ]
    }
   ],
   "source": [
    "# Find all states of game doing a search tree if player 2 starts\n",
    "first_player = -1\n",
    "%time states_actions_player_2 = bfs_cannonical(game, board, first_player) #(-1 black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_actions = {**states_actions_player_1, **states_actions_player_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 53651\n",
      "Number of states: 53651\n"
     ]
    }
   ],
   "source": [
    "print('Number of states:', len(states_actions_player_1))\n",
    "print('Number of states:', len(states_actions_player_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 91282\n"
     ]
    }
   ],
   "source": [
    "print('Number of states:', len(states_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('states_actions', states_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 usuario usuario 144M mar 16 12:38 states_actions.npy\n"
     ]
    }
   ],
   "source": [
    "# Size of the file. Only works if linux or mac. Try dir for windows\n",
    "! ls -lah states_actions.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "\n",
      "Actions, rewards and next nodes of state:\n",
      "1 {'winner': 0, 'next_node': (0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)}\n",
      "4 {'winner': 0, 'next_node': (0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)}\n",
      "11 {'winner': 0, 'next_node': (0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0)}\n",
      "14 {'winner': 0, 'next_node': (0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0)}\n"
     ]
    }
   ],
   "source": [
    "state = list(states_actions.keys())[0]\n",
    "print('Example of state:')\n",
    "print(state)\n",
    "print()\n",
    "print('Actions, rewards and next nodes of state:')\n",
    "for k, v in states_actions[state].items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 1, 0, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5c32d8cba912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m [ 0,  0,  0,  0]])\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mstates_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboard_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: (0, 1, 0, 0, 0, 1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)"
     ]
    }
   ],
   "source": [
    "# Board after player 1 plays action 1\n",
    "board_1 = np.array(\n",
    "[[0,  1,  0,  0],\n",
    "[ 0,  1,  1,  0],\n",
    "[ 0,  1, -1,  0],\n",
    "[ 0,  0,  0,  0]])\n",
    "states_actions[tuple(board_1.reshape(-1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why the above is not a valid state? Which is the state for the Board after player 1 plays action 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a uniform stochastic policy\n",
    "It is used as initial policy to test stochastic policy evalution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_actions = np.load('states_actions.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "Actions, Probabilities:\n",
      "1 0.25\n",
      "4 0.25\n",
      "11 0.25\n",
      "14 0.25\n",
      "------------------\n",
      "Example of state:\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)\n",
      "Actions, Probabilities:\n",
      "0 0.3333333333333333\n",
      "2 0.3333333333333333\n",
      "8 0.3333333333333333\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "from dynamic_programming import generate_uniform_stochastic_policy\n",
    "pi = generate_uniform_stochastic_policy(states_actions)\n",
    "\n",
    "for i in range(2):\n",
    "    state = list(pi.keys())[i]\n",
    "    print('Example of state:')\n",
    "    print(state)\n",
    "    print('Actions, Probabilities:')\n",
    "    for k, v in pi[state].items():\n",
    "        print(k, v)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.25, 4: 0.25, 11: 0.25, 14: 0.25}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi[(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation test\n",
    "### Stochastic policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import policy_evaluation, stochastic_policy_eval_step\n",
    "# stochastic_policy_eval_step does policy_evaluation using probabilities. Check library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \n"
     ]
    }
   ],
   "source": [
    "# Run it multiple times to check it takes different number of iterations to converge\n",
    "stochastic_pi = generate_uniform_stochastic_policy(states_actions)\n",
    "V_stochastic, iters = policy_evaluation(stochastic_policy_eval_step, \n",
    "                             states_actions, \n",
    "                             stochastic_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 examples of Value function:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.19926756827206293\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 0.19926756827206293\n",
      "(0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 0.19926756827206293\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0) 0.1992675682720629\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0) 0.19926756827206293\n",
      "(-1, 1, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.7334317552457347\n",
      "(0, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 0.10847170895012459\n",
      "(0, 1, 0, 0, 0, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 0.027157341479421185\n",
      "(-1, 0, 0, 0, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -0.7334317552457347\n",
      "(0, 0, -1, 0, 1, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 0.027157341479421192\n",
      "(0, 0, 0, 0, 1, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 0.10847170895012459\n"
     ]
    }
   ],
   "source": [
    "print('10 examples of Value function:')\n",
    "i = 0 \n",
    "for k, v in V_stochastic.items():\n",
    "    print(k, v)\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import generate_deterministic_policy, deterministic_policy_eval_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of state: (0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)\n",
      "\"best\" action: 4\n",
      "------------------\n",
      "Example of state: (0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0)\n",
      "\"best\" action: 2\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "deterministic_pi = generate_deterministic_policy(states_actions)\n",
    "for i in range(2):\n",
    "    state = list(pi.keys())[i]\n",
    "    print('Example of state:', state)\n",
    "    print('\"best\" action:', deterministic_pi[state])\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deterministic_pi[(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n"
     ]
    }
   ],
   "source": [
    "# Run it multiple times to check it always takes the same number of iterations\n",
    "V_deterministic, iters = policy_evaluation(deterministic_policy_eval_step, \n",
    "                             states_actions, \n",
    "                             deterministic_pi, 1e-8, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 examples of Value function:\n",
      "(0, 0, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -1\n",
      "(0, -1, 0, 0, 0, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) -1\n",
      "(0, 0, 0, 0, -1, -1, -1, 0, 0, -1, 1, 0, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0) 1\n",
      "(0, 0, 0, 0, 0, 1, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0) -1e-06\n",
      "(-1, 1, 0, 0, 0, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 1e-06\n",
      "(0, 1, -1, 0, 0, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) 1\n",
      "(0, 1, 0, 0, 0, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) -1\n",
      "(-1, 0, 0, 0, 1, -1, 1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -1\n",
      "(0, 0, -1, 0, 1, 1, -1, 0, 0, 1, -1, 0, 0, 0, 0, 0) -1\n",
      "(0, 0, 0, 0, 1, 1, 1, 0, -1, -1, -1, 0, 0, 0, 0, 0) 1\n"
     ]
    }
   ],
   "source": [
    "print('10 examples of Value function:')\n",
    "i = 0 \n",
    "for k, v in V_deterministic.items():\n",
    "    print(k, v)\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See values are just 1 or -1 (They could be +-0.2 for ties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynamic_programming import policy_improve, policy_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 23010\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 4100\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n",
      "Number of differences of new policy vs old policy: 1092\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 271\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 66\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 18\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 9\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 3\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 1\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 2\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 1\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_policy = generate_deterministic_policy(states_actions)\n",
    "optimum_policy, optimum_V = policy_iteration(states_actions, initial_policy, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_1\n",
    "first_player = 1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result indicates that player 1 always looses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_2\n",
    "first_player = -1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Value_func_only_winner', optimum_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pi_func_only_winner', optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 usuario usuario  80M feb 23 02:50 pi_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 12:40 pi_func_only_winner.npy\n",
      "-rw-r--r-- 1 usuario usuario 272M mar 16 10:17 rook_final.npy\n",
      "-rw-rw-r-- 1 usuario usuario 144M mar 16 12:38 states_actions.npy\n",
      "-rw-rw-r-- 1 usuario usuario  133 mar 16 10:16 Value_func_margin_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M feb 23 02:50 Value_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M mar 16 12:39 Value_func_only_winner.npy\n",
      "-rw-rw-r-- 1 usuario usuario  133 mar 16 10:16 Value_func_steps_reward.npy\n"
     ]
    }
   ],
   "source": [
    "! ls -lah *.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets play game with optimun policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_policy = np.load('pi_func_only_winner.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalPolicy = EvaluatePolicy(optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random vs Random as and Greedy vs Greedy as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces):\n",
    "    print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "    print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "    print('ties:', str(int(100*ties/episodes + 0.5))+ '%')\n",
    "    print('Max, Mean, Min margins: ', end ='')\n",
    "    print(np.max(margins), np.mean(margins), np.min(margins))\n",
    "    print('Max, Mean, Min steps: ', end ='')\n",
    "    print(np.max(steps_array), np.mean(steps_array), np.min(steps_array))\n",
    "    print('Max, Mean, Min pieces: ', end ='')\n",
    "    print(np.max(pieces), np.mean(pieces), np.min(pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 35%\n",
      "player_2 wins: 56%\n",
      "ties: 9%\n",
      "Max, Mean, Min margins: 16 -1.703 -16\n",
      "Max, Mean, Min steps: 16 12.522 7\n",
      "Max, Mean, Min pieces: 16 15.725 10\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second player seems to have more probability to win playing random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 46%\n",
      "player_2 wins: 46%\n",
      "ties: 8%\n",
      "Max, Mean, Min margins: 15 -0.209 -16\n",
      "Max, Mean, Min steps: 15 12.196 7\n",
      "Max, Mean, Min pieces: 16 15.513 10\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.greedy_player, -1: evalPolicy.greedy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -2 -6.873 -16\n",
      "Max, Mean, Min steps: 14 12.653 10\n",
      "Max, Mean, Min pieces: 16 15.859 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal policy always win as second player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 82%\n",
      "player_2 wins: 18%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: 14 3.404 -9\n",
      "Max, Mean, Min steps: 14 11.633 9\n",
      "Max, Mean, Min pieces: 16 15.128 12\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtimal policy can't win all but does much better than random player 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 67%\n",
      "player_2 wins: 33%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: 14 2.675 -9\n",
      "Max, Mean, Min steps: 14 11.645 9\n",
      "Max, Mean, Min pieces: 16 15.065 12\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.greedy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also does good as player 1 against greedy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
