{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced policy (Maximize Margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the reward being 1 for winning and -1 for loosing, the reward is the number pieces winner is over looser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dynamic_programming import policy_improve, policy_iteration, generate_deterministic_policy, deterministic_policy_eval_step, policy_improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_actions = np.load('states_actions.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(V, states_actions):\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        actions_list = list(actions.keys())\n",
    "        expected_rewards = np.zeros(len(actions_list))\n",
    "        for i, (action, data) in enumerate(actions.items()):\n",
    "            next_state = data['next_node']\n",
    "            # This is the modification\n",
    "            winner = data['winner']\n",
    "            reward = np.array(next_state).sum()\n",
    "            if winner == 0:\n",
    "                expected_rewards[i] = - V[next_state]\n",
    "            else:\n",
    "                # Esto es un nodo terminal\n",
    "                expected_rewards[i] = - reward\n",
    "        pi[state] = actions_list[np.argmax(expected_rewards)]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_policy_eval_step(states_actions, V, pi):\n",
    "    # Evaluation in place (in contrast with evaluation with 2 arrays).\n",
    "    # Needs less memory and converges too\n",
    "    # pi is a dict and pi[s] is the best action for that state. (The most probable action)\n",
    "    delta = 0\n",
    "    for state, actions in states_actions.items():\n",
    "        V_updated = 0\n",
    "        action = pi[state]\n",
    "        next_node = actions[action]['next_node']\n",
    "        # This is the modification\n",
    "        winner = actions[action]['winner']\n",
    "        reward = np.array(next_node).sum()\n",
    "        if winner == 0:\n",
    "            V_updated = V_updated + (-V[next_node])\n",
    "        else:\n",
    "            # Esto es un nodo terminal\n",
    "            V_updated = V_updated - reward\n",
    "        delta = max(delta, np.abs(V_updated - V[state]))\n",
    "        V[state] = V_updated\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 23068\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n",
      "Number of differences of new policy vs old policy: 7107\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 1868\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n",
      "Number of differences of new policy vs old policy: 425\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 96\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 17\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "Number of differences of new policy vs old policy: 2\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_policy = generate_deterministic_policy(states_actions)\n",
    "optimum_policy, optimum_V = policy_iteration(states_actions, \n",
    "                                             initial_policy, \n",
    "                                             deterministic_policy_eval_step, \n",
    "                                             policy_improve,\n",
    "                                             verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_1\n",
    "first_player = 1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('Value_func_diff_reward', optimum_V)\n",
    "#np.save('pi_func_diff_reward', optimum_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets play game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "import numpy as np\n",
    "\n",
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum_policy = np.load('pi_func_only_winner.npy').item()\n",
    "evalPolicy = EvaluatePolicy(optimum_policy)\n",
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -8 -13.264 -16\n",
      "Max, Mean, Min steps: 16 13.68 10\n",
      "Max, Mean, Min pieces: 16 15.608 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')\n",
    "print('Max, Mean, Min margins: ', end ='')\n",
    "print(np.max(margins), np.mean(margins), np.min(margins))\n",
    "print('Max, Mean, Min steps: ', end ='')\n",
    "print(np.max(steps_array), np.mean(steps_array), np.min(steps_array))\n",
    "print('Max, Mean, Min pieces: ', end ='')\n",
    "print(np.max(pieces), np.mean(pieces), np.min(pieces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 95%\n",
      "player_2 wins: 3%\n",
      "ties: 2%\n",
      "Max, Mean, Min margins: 15 8.635 -8\n",
      "Max, Mean, Min steps: 15 13.074 10\n",
      "Max, Mean, Min pieces: 16 15.305 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "print('ties:', str(int(100*ties/episodes + 0.5))+ '%')\n",
    "print('Max, Mean, Min margins: ', end ='')\n",
    "print(np.max(margins), np.mean(margins), np.min(margins))\n",
    "print('Max, Mean, Min steps: ', end ='')\n",
    "print(np.max(steps_array), np.mean(steps_array), np.min(steps_array))\n",
    "print('Max, Mean, Min pieces: ', end ='')\n",
    "print(np.max(pieces), np.mean(pieces), np.min(pieces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
