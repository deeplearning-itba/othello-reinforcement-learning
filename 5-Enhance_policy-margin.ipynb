{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced policy (Maximize Margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the reward being 1 for winning and -1 for loosing, the reward is the number of pieces winner is over looser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dynamic_programming import policy_improve, policy_iteration, generate_deterministic_policy, deterministic_policy_eval_step, policy_improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_actions = np.load('states_actions.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(V, states_actions):\n",
    "    pi = {}\n",
    "    for state, actions in states_actions.items():\n",
    "        actions_list = list(actions.keys())\n",
    "        expected_rewards = np.zeros(len(actions_list))\n",
    "        for i, (action, data) in enumerate(actions.items()):\n",
    "            next_state = data['next_node']\n",
    "            # This is the modification\n",
    "            winner = data['winner']\n",
    "            reward = np.array(next_state).sum()\n",
    "            if winner == 0:\n",
    "                expected_rewards[i] = - V[next_state]\n",
    "            else:\n",
    "                # Esto es un nodo terminal\n",
    "                expected_rewards[i] = - reward\n",
    "        pi[state] = actions_list[np.argmax(expected_rewards)]\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_policy_eval_step(states_actions, V, pi):\n",
    "    # Evaluation in place (in contrast with evaluation with 2 arrays).\n",
    "    # Needs less memory and converges too\n",
    "    # pi is a dict and pi[s] is the best action for that state. (The most probable action)\n",
    "    delta = 0\n",
    "    for state, actions in states_actions.items():\n",
    "        V_updated = 0\n",
    "        action = pi[state]\n",
    "        next_node = actions[action]['next_node']\n",
    "        # This is the modification\n",
    "        winner = actions[action]['winner']\n",
    "        reward = np.array(next_node).sum()\n",
    "        if winner == 0:\n",
    "            V_updated = V_updated + (-V[next_node])\n",
    "        else:\n",
    "            # Esto es un nodo terminal\n",
    "            V_updated = V_updated - reward\n",
    "        delta = max(delta, np.abs(V_updated - V[state]))\n",
    "        V[state] = V_updated\n",
    "    return V, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 \n",
      "Number of differences of new policy vs old policy: 23012\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n",
      "Number of differences of new policy vs old policy: 7131\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n",
      "Number of differences of new policy vs old policy: 1868\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 \n",
      "Number of differences of new policy vs old policy: 440\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 85\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 \n",
      "Number of differences of new policy vs old policy: 18\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 2\n",
      "---------------------------\n",
      "Iteration number:  1 2 3 4 5 6 7 8 9 10 11 12 13 \n",
      "Number of differences of new policy vs old policy: 0\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "initial_policy = generate_deterministic_policy(states_actions)\n",
    "optimum_policy, optimum_V = policy_iteration(states_actions, \n",
    "                                             initial_policy, \n",
    "                                             deterministic_policy_eval_step, \n",
    "                                             policy_improve,\n",
    "                                             verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0]\n",
      " [ 0 -1  1  0]\n",
      " [ 0  1 -1  0]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8\n"
     ]
    }
   ],
   "source": [
    "# Empieza player_1\n",
    "first_player = 1\n",
    "print(optimum_V[tuple(first_player * board.reshape(-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Value_func_margin_reward', optimum_V)\n",
    "np.save('pi_func_margin_reward', optimum_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 12:46 pi_func_margin_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M feb 23 02:50 pi_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario  80M mar 16 12:40 pi_func_only_winner.npy\n",
      "-rw-r--r-- 1 usuario usuario 272M mar 16 10:17 rook_final.npy\n",
      "-rw-rw-r-- 1 usuario usuario 144M mar 16 12:38 states_actions.npy\n",
      "-rw-rw-r-- 1 usuario usuario  81M mar 16 12:45 Value_func_margin_reward.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M feb 23 02:50 Value_func.npy\n",
      "-rw-rw-r-- 1 usuario usuario  76M mar 16 12:39 Value_func_only_winner.npy\n",
      "-rw-rw-r-- 1 usuario usuario  133 mar 16 10:16 Value_func_steps_reward.npy\n"
     ]
    }
   ],
   "source": [
    "!ls -lah *.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets play game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from othello.OthelloGame import OthelloGame as Game\n",
    "from othello.OthelloGame import display as displayGame\n",
    "import numpy as np\n",
    "\n",
    "from playing_stats import EvaluatePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum_policy = np.load('pi_func_margin_reward.npy').item()\n",
    "evalPolicy = EvaluatePolicy(optimum_policy)\n",
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces):\n",
    "    print('player_1 wins:', str(int(100*player_1_wins/episodes + 0.5)) + '%')\n",
    "    print('player_2 wins:', str(int(100*player_2_wins/episodes + 0.5)) +'%')\n",
    "    print('ties:', str(int(100*ties/episodes + 0.5))+ '%')\n",
    "    print('Max, Mean, Min margins: ', end ='')\n",
    "    print(np.max(margins), np.mean(margins), np.min(margins))\n",
    "    print('Max, Mean, Min steps: ', end ='')\n",
    "    print(np.max(steps_array), np.mean(steps_array), np.min(steps_array))\n",
    "    print('Max, Mean, Min pieces: ', end ='')\n",
    "    print(np.max(pieces), np.mean(pieces), np.min(pieces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays second against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -8 -13.318 -16\n",
      "Max, Mean, Min steps: 16 13.758 10\n",
      "Max, Mean, Min pieces: 16 15.636 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.random_player, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy plays first against random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 95%\n",
      "player_2 wins: 3%\n",
      "ties: 2%\n",
      "Max, Mean, Min margins: 15 8.337 -8\n",
      "Max, Mean, Min steps: 15 13.06 10\n",
      "Max, Mean, Min pieces: 16 15.339 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.random_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player1: Optumin Policy (Margin), Player2: Optimun Policy (only win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_func_margin_reward.npy  states_actions.npy\n",
      "pi_func.npy\t\t   Value_func_margin_reward.npy\n",
      "pi_func_only_winner.npy    Value_func.npy\n",
      "pi_func_steps_reward.npy   Value_func_only_winner.npy\n",
      "rook_final.npy\t\t   Value_func_steps_reward.npy\n"
     ]
    }
   ],
   "source": [
    "!ls *.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_margin = np.load('pi_func_margin_reward.npy').item(0)\n",
    "pi_only_wins = np.load('pi_func_only_winner.npy').item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum_policy = np.load('pi_func_only_winner.npy').item()\n",
    "evalPolicy = EvaluatePolicy(pi_margin, pi_only_wins)\n",
    "n = 4\n",
    "game = Game(n)\n",
    "board = game.getInitBoard()\n",
    "player = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -2 -2.0 -2\n",
      "Max, Mean, Min steps: 13 13.0 13\n",
      "Max, Mean, Min pieces: 16 16.0 16\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player, -1: evalPolicy.policy_player_pi2}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has no sense to play more than once because they are deterministic policies\n",
    "- Very interesting to note that player 2 wins but with a margin of only -2, the minimum margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player1: Optimun Policy (only win), Player2: Optumin Policy (Margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player_1 wins: 0%\n",
      "player_2 wins: 100%\n",
      "ties: 0%\n",
      "Max, Mean, Min margins: -9 -9.0 -9\n",
      "Max, Mean, Min steps: 10 10.0 10\n",
      "Max, Mean, Min pieces: 13 13.0 13\n"
     ]
    }
   ],
   "source": [
    "episodes = 1\n",
    "player_1_wins, player_2_wins, ties, margins, steps_array, pieces = evalPolicy.get_stats(game, \n",
    "                                                board, \n",
    "                                                {1: evalPolicy.policy_player_pi2, -1: evalPolicy.policy_player}, \n",
    "                                                episodes)\n",
    "display_results(player_1_wins, player_2_wins, ties, margins, steps_array, pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has no sense to play more than once because they are deterministic policies\n",
    "- Margin policy as second player wins by a margin of 9. Es expected more or equal than 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] [1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.array([1, 2, 3, 4])\n",
    "arr_2 = arr_1[0]\n",
    "arr_2 = 10\n",
    "list_1 = [1, 2, 3, 4]\n",
    "list_2 = list_1[:3]\n",
    "list_2[0] = 10\n",
    "print(list_1, arr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10] None [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.array([10,  2,  8,  7,  6,  5,  4,  1,  9,  3])\n",
    "arr_sorted_1 = arr_1.sort()\n",
    "arr_sorted_2 = np.sort(arr_1)\n",
    "print(arr_1, arr_sorted_1, arr_sorted_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(arr_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  7  5  9]\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.array([10,  2,  8,  7,  6,  5,  4,  1,  9,  3])\n",
    "list_1 = [0, 3, 5, -2]\n",
    "print(arr_1[list_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 4 5 1]\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.array([10,  2,  8,  7,  6,  5])\n",
    "print(np.argsort(arr_1)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  8  7  6  5  2]\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(arr_1)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
